{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import re\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import nltk \n",
    "nltk.download('stopwords')  \n",
    "from nltk.corpus import stopwords \n",
    "from tweepy import OAuthHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to store the Consumer API key, Consumer API secret and Access token and token secret in corresponding string valirables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = 'XXXXXXXXXXX'\n",
    "consumer_secret = 'XXXXXXXXXXXX'\n",
    "access_token = 'XXXXXXXXXXXXXXXXXXX'\n",
    "access_token_secret = 'XXXXXXXXXXXXXXXXXXs' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OAuthHandler takes the Consumer API ke and Customer API Secret as argument.Customer API and Secret key tell our client application which application to connect with, While the Acess token and secret key defines the rights to access the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "authorizer = OAuthHandler(consumer_key, consumer_secret)\n",
    "authorizer.set_access_token(access_token,access_token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping Tweets\n",
    "We have successfully connected to the twitter API. Now we have to fetch the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = tweepy.API(authorizer,timeout=15)\n",
    "all_tweets= []\n",
    "search_query= 'narendra modi'\n",
    "for tweet_object in tweepy.Cursor(api.search,q=search_query+\"-filter:retweets\",lang = 'en', result_type='recent').items(500):\n",
    "    all_tweets.append(tweet_object.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specified that if no tweet are found after seacrhing for 15 seconds then the application should time out. \n",
    "all_tweets contain the scraped tweets.\n",
    "Search_query contains the string which we are looking for in the tweets.\n",
    "In the loop that uses tweepy's cursor object to fetch tweets.\n",
    "1. The first parameter is used to tell us what we are performing(We are seraching the tweets)\n",
    "2. Second parameter is search query where we re provide additional details to it. in our query we are telling it to not to fetch any information form retweets.\n",
    "3. Third parameter is used to specify what kinf of result type we are looking for.\n",
    "4. item attributes sets the number of tweets to return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Sentimental Analysis\n",
    ">Data Processing\n",
    " To train our model we will be using aline data base from Github. We will divide the data into label and feature set and then will remove special charcter and empty spaces from the tweets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets =pd.read_csv(\"https://raw.githubusercontent.com/kolaveridi/kaggle-Twitter-US-Airline-Sentiment-/master/Tweets.csv\")\n",
    "\n",
    "X = tweets.iloc[:, 10].values  \n",
    "y = tweets.iloc[:, 1].values\n",
    " \n",
    "processed_tweets = []\n",
    " \n",
    "for tweet in range(0, len(X)):  \n",
    "    # Remove all the special characters\n",
    "    processed_tweet = re.sub(r'\\W', ' ', str(X[tweet]))\n",
    " \n",
    "    # remove all single characters\n",
    "    processed_tweet = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_tweet)\n",
    " \n",
    "    # Remove single characters from the start\n",
    "    processed_tweet = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_tweet) \n",
    " \n",
    "    # Substituting multiple spaces with single space\n",
    "    processed_tweet= re.sub(r'\\s+', ' ', processed_tweet, flags=re.I)\n",
    " \n",
    "    # Removing prefixed 'b'\n",
    "    processed_tweet = re.sub(r'^b\\s+', '', processed_tweet)\n",
    " \n",
    "    # Converting to Lowercase\n",
    "    processed_tweet = processed_tweet.lower()\n",
    " \n",
    "    processed_tweets.append(processed_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF for text to Numeric Conversion\n",
    "\n",
    "TFIDF scheme converts text to numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "tfidfconverter = TfidfVectorizer(max_features=2000, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))  \n",
    "X = tfidfconverter.fit_transform(processed_tweets).toarray()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Sentimental Analysis Model\n",
    "We will train our model to predict the tweets which we have extrated above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "text_classifier = RandomForestClassifier(n_estimators=100, random_state=0)  \n",
    "text_classifier.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Sentiment for Scraped Tweets:\n",
    "We will be running above algorithm on the scraped tweets which we have generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "39 120 341\n",
      "People on narendra modi have neutral opinion of 68.2 %\n"
     ]
    }
   ],
   "source": [
    "p = 0\n",
    "ni = 0\n",
    "n = 0\n",
    "for tweet in all_tweets:\n",
    "    processed_tweet = re.sub(r'\\W', ' ', tweet)\n",
    " \n",
    "    # remove all single characters\n",
    "    processed_tweet = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_tweet)\n",
    " \n",
    "    # Remove single characters from the start\n",
    "    processed_tweet = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_tweet) \n",
    " \n",
    "    # Substituting multiple spaces with single space\n",
    "    processed_tweet= re.sub(r'\\s+', ' ', processed_tweet, flags=re.I)\n",
    " \n",
    "    # Removing prefixed 'b'\n",
    "    processed_tweet = re.sub(r'^b\\s+', '', processed_tweet)\n",
    " \n",
    "    # Converting to Lowercase\n",
    "    processed_tweet = processed_tweet.lower()\n",
    " \n",
    "    sentiment = text_classifier.predict(tfidfconverter.transform([ processed_tweet]).toarray())\n",
    "    if sentiment == ['positive']:\n",
    "        p = p +1\n",
    "    elif sentiment == ['negative']:\n",
    "        ni = ni +1\n",
    "    else :\n",
    "        n = n + 1\n",
    "   # print(processed_tweet ,\":\", sentiment)\n",
    "# print(len(all_tweets))\n",
    "# print( p,ni,n)\n",
    "if p>ni and p > n :\n",
    "    print(\"People on\",search_query,\"have positive opinion \",(p/(p+n+ni)*100),\"%\")\n",
    "elif ni >p and ni > n:\n",
    "    print(\"People on\",search_query,\"have negative opinion of\",(ni/(p+n+ni)*100),\"%\")\n",
    "elif n> p and n> ni :\n",
    "    print(\"People on\",search_query,\"have neutral opinion of\",(n/(p+n+ni)*100),\"%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
